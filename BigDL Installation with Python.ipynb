{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Intel BigDL with DSX on Cloud in Python\n\nUsers can install and use a recent version of BigDL themselves.\nFor Python notebooks, this requires two steps, which are explained in detail below:\n1. Install a JAR file into the default classpath.\n2. Install the Python package.\n\nInstructions below are for notebooks running in DSX on Cloud, backed by Apache Spark as a Service.  \nInstructions have also been [posted on StackOverflow.com](https://stackoverflow.com/a/47282619/5629418) now.\n\n## Preparation\n\nYou need to select a matching combination of Python, Spark, and BigDL.\nThe notebook kernel in DSX on Cloud determines Python and Spark versions.\nThis notebook uses Python 3.5 with Spark 2.1.\n\nCheck the BigDL [download page](https://bigdl-project.github.io/master/#release-download/)\nfor a recent release that supports your chosen version of Spark. The fixlevel does not matter.\nAt the time of writing, release 0.3.0 is the newest, and it does support Spark 2.1.\n**Note:** Do not download the package, just pick the BigDL version you want to use.\n\nInstallation with Python 2.7 works exactly like with Python 3.5.\nIf you want to switch Python versions during development,\nyou have to install the Python package twice, once for each version.\nSwitching between Spark versions will not work, because they use the same classpath\nand you can only put one BigDL JAR there.\n  \n\n## Cleanup\n\nMake sure that BigDL isn't installed yet. If it is, delete it before installing a new version.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# JAR files... if the output is empty, none are in the way\n!find ~/data/libs -name bigdl-\\*", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Remove BigDL JAR files...\n!find ~/data/libs -name bigdl-\\* -exec rm -vf {} +", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Python package... if the output is empty, it's not installed.\n!pip freeze | grep -i BigDL", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Remove the user-installed BigDL Python package...\n!rm -rf ~/.local/lib/python${_py_version_}/site-packages/{bigdl,BigDL}*\n# ${_py_version_} stands for your Python version, 3.5 or 2.7\n\n# Remove possible leftovers from a recent installation attempt...\n# https://stackoverflow.com/q/47179822\n!rm -rf $PIP_BUILD", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "If you had to clean up an old installation of BigDL, restart the kernel now.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Installation\n\nYou need a JAR file with dependencies. This can be downloaded from a Maven repository. The URL depends on the versions of Spark and BigDL. Each version appears twice, once in the path and once in the filename. For example, the download link for Spark 2.1 and BigDL 0.3.0 is\n\n    https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-SPARK_2.1/0.3.0/bigdl-SPARK_2.1-0.3.0-jar-with-dependencies.jar\n\nPut the JAR in `~/data/libs/` and it will be found.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# modify the versions of Spark (sv) and BigDL (bv) as required, the URL will adjust automatically...\n!(export sv=2.1 bv=0.3.0 ; cd ~/data/libs/ && wget  https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-SPARK_${sv}/${bv}/bigdl-SPARK_${sv}-${bv}-jar-with-dependencies.jar)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You need the Python package for the BigDL version of the JAR. It can be installed from PyPI with `pip`.\nThe `--user` flag is automatically provided by the environment.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install bigdl==0.3.0 | cat", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Restart the notebook kernel and BigDL is ready for use.\n\n# Use BigDL\n\nWhen importing bigdl, some output about paths is expected.\nA warning about SPARK_HOME and pyspark is safe to ignore.\n\nThe example below is adapted from the \"Forward and backward\" tutorial at:  \nhttps://github.com/intel-analytics/BigDL-tutorials\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# https://github.com/intel-analytics/BigDL-Tutorials/blob/master/notebooks/neural_networks/forward_and_backward.ipynb\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Prepending /gpfs/fs01/user/s292-2479f4c3f28697-e04c173fe4bd/.local/lib/python3.5/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/gpfs/fs01/user/s292-2479f4c3f28697-e04c173fe4bd/.local/lib/python3.5/site-packages/bigdl/util/engine.py:39: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /usr/local/src/spark21master/spark-2.1.2-bin-2.7.3, and pyspark is found in: /gpfs/fs01/user/s292-2479f4c3f28697-e04c173fe4bd/.local/lib/python3.5/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n  warnings.warn(warning_msg)\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "linear = Linear(2, 1)\nprint (linear.parameters())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "creating: createLinear\n{'Linear2023adc': {'gradBias': array([0.], dtype=float32), 'weight': array([[-0.33976966,  0.10730403]], dtype=float32), 'gradWeight': array([[0., 0.]], dtype=float32), 'bias': array([-0.3070345], dtype=float32)}}\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "input = np.array([1,-2])\n# forward to output\noutput = linear.forward(input)\nprint (output)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[-0.8614122]\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "# mean absolute error\nmae = AbsCriterion()\ntarget = np.array([0])\n\nloss = mae.forward(output, target)\nprint(\"loss: \" + str(loss))\n        \ngrad_output = mae.backward(output, target)\nlinear.backward(input, grad_output)\n\nprint (linear.parameters())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "creating: createAbsCriterion\nloss: 0.8614122\n{'Linear2023adc': {'gradBias': array([-1.], dtype=float32), 'weight': array([[-0.33976966,  0.10730403]], dtype=float32), 'gradWeight': array([[-1.,  2.]], dtype=float32), 'bias': array([-0.3070345], dtype=float32)}}\n"
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}