{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LeNet model with MNIST dataset using BigDL in IBM DSX\n",
    "\n",
    "#### Companion notebook for [this blog post](https://medium.com/@hhbyyh/training-lenet-model-with-mnist-dataset-using-bigdl-in-ibm-dsx-7e8310b23057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!(export sv=2.1 bv=0.3.0 ; cd ~/data/libs/ && wget  https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-SPARK_${sv}/${bv}/bigdl-SPARK_${sv}-${bv}-jar-with-dependencies.jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bigdl==0.3.0 | cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepending /gpfs/fs01/user/s12c-e1e4611360e57a-511d20592d9b/.local/lib/python2.7/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/fs01/user/s12c-e1e4611360e57a-511d20592d9b/.local/lib/python2.7/site-packages/bigdl/util/engine.py:39: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /usr/local/src/spark21master/spark-2.1.2-bin-2.7.3, and pyspark is found in: /gpfs/fs01/user/s12c-e1e4611360e57a-511d20592d9b/.local/lib/python2.7/site-packages/pyspark/__init__.pyc. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.util.common import *\n",
    "from pyspark import SparkContext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "confCore=create_spark_conf()\n",
    "confCore.set(\"spark.executor.cores\", 1)\n",
    "confCore.set(\"spark.cores.max\", 1)\n",
    "sc = SparkContext(appName=\"Mnist\", conf=confCore)\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createLinear\n",
      "{u'Linear153a34c0': {u'gradWeight': array([[ 0.,  0.]], dtype=float32), u'bias': array([ 0.24533921], dtype=float32), u'weight': array([[-0.50188249, -0.1742793 ]], dtype=float32), u'gradBias': array([ 0.], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(2, 1)\n",
    "print (linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optparse import OptionParser\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', 'train-images-idx3-ubyte.gz')\n",
      "('Extracting', 'train-labels-idx1-ubyte.gz')\n",
      "('Extracting', 't10k-images-idx3-ubyte.gz')\n",
      "('Extracting', 't10k-labels-idx1-ubyte.gz')\n",
      "creating: createSequential\n",
      "creating: createReshape\n",
      "creating: createSpatialConvolution\n",
      "creating: createTanh\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createTanh\n",
      "creating: createSpatialConvolution\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createReshape\n",
      "creating: createLinear\n",
      "creating: createTanh\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createClassNLLCriterion\n",
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n",
      "training finished\n"
     ]
    }
   ],
   "source": [
    "def build_model(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape([1, 28, 28]))\n",
    "    model.add(SpatialConvolution(1, 6, 5, 5))\n",
    "    model.add(Tanh())\n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "    model.add(Tanh())\n",
    "    model.add(SpatialConvolution(6, 12, 5, 5))\n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "    model.add(Reshape([12 * 4 * 4]))\n",
    "    model.add(Linear(12 * 4 * 4, 100))\n",
    "    model.add(Tanh())\n",
    "    model.add(Linear(100, class_num))\n",
    "    model.add(LogSoftMax())\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_mnist(sc, data_type=\"train\", location=\"/tmp/mnist\"):\n",
    "    \"\"\"\n",
    "    Get and normalize the mnist data. We would download it automatically\n",
    "    if the data doesn't present at the specific location.\n",
    "    :param sc: SparkContext\n",
    "    :param data_type: training data or testing data\n",
    "    :param location: Location storing the mnist\n",
    "    :return: A RDD of (features: Ndarray, label: Ndarray)\n",
    "    \"\"\"\n",
    "    (images, labels) = mnist.read_data_sets(location, data_type)\n",
    "    images = sc.parallelize(images)\n",
    "    labels = sc.parallelize(labels + 1) # Target start from 1 in BigDL\n",
    "    record = images.zip(labels)\n",
    "    return record\n",
    "\n",
    "def get_end_trigger():\n",
    "        return MaxEpoch(10)\n",
    "\n",
    "train_data = get_mnist(sc, \"train\", \"\")\\\n",
    "    .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n",
    "                       rec_tuple[1]))\\\n",
    "    .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n",
    "test_data = get_mnist(sc, \"test\", \"\")\\\n",
    "    .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TEST_MEAN, mnist.TEST_STD),\n",
    "                       rec_tuple[1]))\\\n",
    "    .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n",
    "optimizer = Optimizer(\n",
    "    model=build_model(10),\n",
    "    training_rdd=train_data,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002),\n",
    "    end_trigger=get_end_trigger(),\n",
    "    batch_size=128)\n",
    "optimizer.set_validation(\n",
    "    batch_size=128,\n",
    "    val_rdd=test_data,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[Top1Accuracy()]\n",
    ")\n",
    "trained_model = optimizer.optimize()\n",
    "parameters = trained_model.parameters()\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTop1Accuracy\n",
      "Evaluated result: 0.946799993515, total_num: 10000, method: Top1Accuracy\n"
     ]
    }
   ],
   "source": [
    "results = trained_model.evaluate(test_data, 128, [Top1Accuracy()])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
